{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "from IPython.display import clear_output\n",
                "import time\n",
                "from Environments.environments import train_envs, get_all_avail_envs, get_all_avail_test_envs, test_envs\n",
                "from Agents.dqn_agent import DQN_Agent\n",
                "from Agents.ppo_agent import PPO_Agent\n",
                "from Models import fc, rnn\n",
                "import matplotlib.pyplot as plt\n",
                "import utils\n",
                "from Curriculum_managers.random_curriculum import Random_Curriculum\n",
                "from Curriculum_managers.paired_curriculum import PAIRED_Curriculum\n",
                "from Curriculum_managers.paired_curriculum_or import PAIRED_Curriculum_OR\n",
                "from Curriculum_managers.paired_curriculum_hf import PAIRED_Curriculum_History_filter\n",
                "from Curriculum_managers.paired_curriculum_hfe import PAIRED_Curriculum_History_filter_Entropy\n",
                "from Curriculum_managers.paired_curriculum_or_hfe import PAIRED_Curriculum_Original_R_History_filter_Entropy\n",
                "from Curriculum_managers.curriculum_no_regulator_he import Curriculum_Unregulated_Entropy_History\n",
                "\n",
                "import plotly.express as px\n",
                "import numpy as np\n",
                "import plotly.io as pio\n",
                "import cv2\n",
                "pio.renderers.default = \"browser\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_res(vec, title):\n",
                "    fig = px.line(x=range(len(np.convolve(vec, np.ones(100)/100,mode='valid'))), y=np.convolve(vec, np.ones(100)/100, mode='valid'), title=title)\n",
                "    fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['SingleTaxiEnv', 'FrozenLakeEnv', 'AdversarialEnv', 'ReparameterizedAdversarialEnv', 'MiniAdversarialEnv', 'MiniReparameterizedAdversarialEnv', 'NoisyAdversarialEnv', 'MediumAdversarialEnv', 'GoalLastAdversarialEnv', 'MiniGoalLastAdversarialEnv', 'Sokoban']\n"
                    ]
                }
            ],
            "source": [
                "device = utils.init_torch()\n",
                "env_names = get_all_avail_envs()\n",
                "print(env_names[:15])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "define n_iters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_iters = 50000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = train_envs['MiniAdversarialEnv']()\n",
                "n_actions = env.get_action_space().n\n",
                "obs_shape = env.get_observation_space()\n",
                "gen_obs_shape = env.get_generator_observation_space()\n",
                "gen_action_dim = env.get_generator_action_space().n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RANDOM CURRICULUM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "loading metadatafrom ./results/Random_Curriculum/MiniAdversarialEnv/meta_data.pkl\n",
                        "loading models from last iter: 299\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "0it [00:00, ?it/s]\n"
                    ]
                }
            ],
            "source": [
                "r_agent = PPO_Agent(obs_shape, n_actions, device=device, batch_size=64, max_mem_size=10**5,num_parallel_envs=4, lr=0.0001, model=rnn.RNN)\n",
                "r_teacher = Random_Curriculum(env ,trainee=r_agent)\n",
                "r_rewards = r_teacher.teach(n_iters=n_iters, n_episodes=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PAIRED CURRICULUM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "no files to load from\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "R:000000.0, entropy: 1.941: 100%|██████████| 20/20 [00:06<00:00,  2.94it/s]\n"
                    ]
                }
            ],
            "source": [
                "p_agent = PPO_Agent(obs_shape, n_actions, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "teacher_agent = PPO_Agent(gen_obs_shape, gen_action_dim, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "p_teacher = PAIRED_Curriculum(env, teacher_agent=teacher_agent ,trainee=p_agent)\n",
                "p_rewards = p_teacher.teach(n_iters=n_iters, n_episodes=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PAIRED CURRICULUM OR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "no files to load from\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "R:000000.0, entropy: 1.942: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s]\n"
                    ]
                }
            ],
            "source": [
                "por_agent = PPO_Agent(obs_shape, n_actions, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "por_teacher_agent = PPO_Agent(gen_obs_shape, gen_action_dim, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "por_teacher = PAIRED_Curriculum_OR(env, teacher_agent=por_teacher_agent ,trainee=por_agent)\n",
                "por_rewards = por_teacher.teach(n_iters=n_iters, n_episodes=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PAIRED History Filter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "no files to load from\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "R:000000.0, entropy: 1.943: 100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n"
                    ]
                }
            ],
            "source": [
                "phf_agent = PPO_Agent(obs_shape, n_actions, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "phf_teacher_agent = PPO_Agent(gen_obs_shape, gen_action_dim, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "phf_teacher = PAIRED_Curriculum_History_filter(env, teacher_agent=phf_teacher_agent ,trainee=phf_agent)\n",
                "phf_rewards = phf_teacher.teach(n_iters=20, n_episodes=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PAIRED History Filter Entropy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "no files to load from\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "R:000000.0, entropy: 1.944: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n"
                    ]
                }
            ],
            "source": [
                "phfe_agent = PPO_Agent(obs_shape, n_actions, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "phfe_teacher_agent = PPO_Agent(gen_obs_shape, gen_action_dim, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "phfe_teacher = PAIRED_Curriculum_History_filter_Entropy(env, teacher_agent=phfe_teacher_agent ,trainee=phfe_agent)\n",
                "phfe_rewards = phfe_teacher.teach(n_iters=20, n_episodes=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PAIRED History Filter Entropy OR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "no files to load from\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "R:00000.16, entropy: 1.93: 100%|██████████| 20/20 [00:21<00:00,  1.10s/it] \n"
                    ]
                }
            ],
            "source": [
                "phfe_or_agent = PPO_Agent(obs_shape, n_actions, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "phfe_or_teacher_agent = PPO_Agent(gen_obs_shape, gen_action_dim, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "phfe_or_teacher = PAIRED_Curriculum_Original_R_History_filter_Entropy(env, teacher_agent=phfe_or_teacher_agent ,trainee=phfe_or_agent)\n",
                "phfe_rewards = phfe_or_teacher.teach(n_iters=20, n_episodes=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# UNREGULATED ENTROPY HISTORY"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "no files to load from\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "R:-00127.5, entropy: 1.941: 100%|██████████| 20/20 [00:22<00:00,  1.15s/it]\n"
                    ]
                }
            ],
            "source": [
                "pee_agent = PPO_Agent(obs_shape, n_actions, device=device, batch_size=64, max_mem_size=10**5, lr=0.0001, model=rnn.RNN)\n",
                "pee_teacher_agent = PPO_Agent(gen_obs_shape, gen_action_dim, device=device, batch_size=64, max_mem_size=10**5 ,lr=0.0001, model=rnn.RNN)\n",
                "pee_teacher = Curriculum_Unregulated_Entropy_History(env, teacher_agent=pee_teacher_agent ,trainee=pee_agent, inv_reward_entropy_coeff=1)\n",
                "pee_rewards = pee_teacher.teach(n_iters=20, n_episodes=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "e = pee_teacher.create_envs(1)[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "o = e.reset()\n",
                "plt.imshow(e.render('rgb_array'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_res(pee_teacher.agent_train_entropy, \"trainee_entropy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = pee_teacher.create_envs(1)[0]\n",
                "plt.imshow(env.render('rgb_array'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RESULTS CHECK"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = pee_teacher.create_envs(1)[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env.reset_random()\n",
                "plt.imshow(env.render('rgb_array'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "domain_name = \"Taxi\"\n",
                "difficulty = \"easy\"\n",
                "get_all_avail_test_envs(domain_name, difficulty)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = test_envs[\"Cluttered40Minigrid\"]()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#pen_agent, p_agent, r_agent, pee_agent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "agent = pee_agent\n",
                "agent.set_eval_mode()\n",
                "vid = []\n",
                "for i in range(1):\n",
                "    obs = env.reset()\n",
                "    # break\n",
                "    R = 0\n",
                "    t = 0\n",
                "    while True:\n",
                "        # Uncomment to watch the behavior in a GUI window\n",
                "        clear_output(wait=True)\n",
                "        frame = env.render('rgb_array')\n",
                "        vid.append(frame)\n",
                "        plt.imshow(frame)\n",
                "        plt.show()\n",
                "        # env.render(mode='rgb_array')\n",
                "        # env.render()\n",
                "        action = agent.act(obs)\n",
                "        obs, r, done, _ = env.step(action)\n",
                "        print(action, r, done)\n",
                "\n",
                "        R += r\n",
                "        t += 1\n",
                "        reset = t == 500\n",
                "        time.sleep(0.01)\n",
                "\n",
                "        if done or reset:\n",
                "            break\n",
                "    print('evaluation episode:', i, 'R:', R)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save_vid(vid,'test')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_vid(frames, vid_name):\n",
                "    out = cv2.VideoWriter(vid_name+'.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, (800,640))\n",
                " \n",
                "    for i in range(len(frames)):\n",
                "        frame = cv2.cvtColor(frames[i], cv2.COLOR_RGB2BGR)\n",
                "        frame = cv2.resize(frame, (800,640))\n",
                "        out.write(frame)\n",
                "    out.release()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Curriculum results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_agent(agent, env, num_iters=1, plot=False):\n",
                "    agent.set_eval_mode()\n",
                "    mean_r = 0\n",
                "    for i in range(num_iters):\n",
                "        obs = env.reset()\n",
                "        # break\n",
                "        R = 0\n",
                "        t = 0\n",
                "        while True:\n",
                "            # Uncomment to watch the behavior in a GUI window\n",
                "            if plot:\n",
                "                clear_output(wait=True)\n",
                "                plt.imshow(env.render('rgb_array'))\n",
                "                plt.show()\n",
                "                time.sleep(0.01)\n",
                "\n",
                "            # env.render(mode='rgb_array')\n",
                "            # env.render()\n",
                "            action = agent.act(obs)\n",
                "            obs, r, done, _ = env.step(action)\n",
                "\n",
                "            R += r\n",
                "            t += 1\n",
                "            reset = t == 500\n",
                "\n",
                "            if done or reset:\n",
                "                break\n",
                "        mean_r +=R\n",
                "    return mean_r / num_iters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "runable_envs = {}\n",
                "for env_name in get_all_avail_test_envs():\n",
                "    try:\n",
                "        test_env = test_envs[env_name]()\n",
                "        runable_envs[env_name] = test_env\n",
                "    except:\n",
                "        print(\"Exception\",env_name, test_env.minigrid_mode)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "agents = {\"paired_entropy\": pen_agent, \"paired\":p_agent, \"random\":r_agent, \"entropy_only\":pee_agent, \"entropy_history\": peh_agent, \"paired_entropy_history\": penh_agent}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calc_result(agents, runable_envs):\n",
                "    i=0\n",
                "    all_results = np.zeros((len(runable_envs), len(agents)))\n",
                "    for env_name,env in runable_envs.items():\n",
                "        print(env_name, i, len(runable_envs))\n",
                "        j= 0\n",
                "        for a_name, a in agents.items():\n",
                "            env.__init__()\n",
                "            res = run_agent(a, env)\n",
                "            all_results[i][j] = res\n",
                "            j+=1\n",
                "        i+=1\n",
                "    return all_results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_results = calc_result(agents, runable_envs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clean_env_names = np.array(list(runable_envs.keys()))[~np.all(all_results == 0, axis=1)]\n",
                "clean_res = all_results[~np.all(all_results == 0, axis=1)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_test_res_all(clean_res, clean_env_names, agents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print_mean_score(all_results, agents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_mean_score(all_results, agents):\n",
                "    scores = np.mean(all_results, axis=0)\n",
                "    for i,a in enumerate(agents):\n",
                "        print(f\"{a}_mean:\", scores[i])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from plotly.subplots import make_subplots\n",
                "from plotly import graph_objects as go\n",
                "def plot_test_res_all(data_array, env_names, agents_dict):\n",
                "    color10_16 = ['violet', 'orange', 'cyan', \"green\",  \"yellow\"]\n",
                "    col_len = int(np.sqrt(len(env_names)))\n",
                "    row_len = (col_len + len(env_names) - col_len**2)\n",
                "    fig = make_subplots(rows=row_len, cols=col_len, subplot_titles=env_names)\n",
                "    for i,env_name in enumerate(env_names):\n",
                "        fig.add_trace(\n",
                "            go.Bar(\n",
                "                name=env_name,\n",
                "                x=list(agents_dict.keys()),\n",
                "                y=data_array[i],\n",
                "                marker_color=color10_16\n",
                "            ),\n",
                "            row=(i // col_len) +1 ,\n",
                "            col =(i % col_len)+1,\n",
                "            \n",
                "        )\n",
                "    fig.update_layout(height=1600, width=1600, title_text=\"Reward Comparation\", showlegend=False, legend=dict(\n",
                "    yanchor=\"bottom\",\n",
                "    y=-0.5,\n",
                "    xanchor=\"right\",\n",
                "    x=1\n",
                "))\n",
                "    fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# FOR DEBUG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "o=env.reset()\n",
                "agent = pee_agent\n",
                "agent.reset_rnn_hidden()\n",
                "# o = env.reset()\n",
                "v= agent.pre_process_obs_for_act(o,1)\n",
                "agent.critic_model(v)\n",
                "\n",
                "o,_,_,_ = env.step(2)\n",
                "plt.imshow(env.render('rgb_array'))"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "6939f772228930f094315145e416d5954b20b1f6473e0de1ef78293fcab749f1"
        },
        "kernelspec": {
            "display_name": "Python 3.9.7 64-bit ('venv': venv)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.9"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
